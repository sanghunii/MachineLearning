{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ff5609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4157, 3) (1040, 3)\n",
      "0.9971133028626413\n",
      "0.864423076923077\n",
      "{'fit_time': array([0.00984097, 0.00789809, 0.00699997, 0.00680614, 0.00644493]), 'score_time': array([0.00064492, 0.00052094, 0.00043917, 0.00044298, 0.00042892]), 'test_score': array([0.87019231, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}\n",
      "final score of model :  0.8554925223957948\n",
      "0.8554925223957948\n",
      "0.8581873425226026\n",
      "0.9615162593804117\n",
      "{'min_impurity_decrease': 0.0001}\n",
      "[0.86800067 0.86453617 0.86492226 0.86780891 0.86761605]\n",
      "{'min_impurity_decrease': 0.0001}\n",
      "{'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}\n",
      "0.8683865773302731\n",
      "{'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}\n",
      "0.8695428296438884\n",
      "0.86\n"
     ]
    }
   ],
   "source": [
    "##Validation set\n",
    "\n",
    "#Until now, we use test_set for evaluate model. \n",
    "#However, if you repeatedly use test_set and find the optimal model, there is a disadvantage that a model that fits test_set is created.\n",
    "#So from now, we use validation set. We can separate train set fot make this like test_set.\n",
    "#Example\n",
    "    #Separate 20% from train_set for test_set and again 20% for validation set. \n",
    "    #Find optimal hyperparameter use train_set(60%), validation set(20%)\n",
    "    #After find, give the final score as train_set + validation set(80%)  and test_set(20%)\n",
    "\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
    "\n",
    "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()  #2-dimension array\n",
    "target = wine['class'].to_numpy()                  #linear array.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#separate test_data from whole (20% for test, 80% for train data)\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#separate validation_data from remain train_data (20% of train data(it is 80% of all data))\n",
    "sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(sub_input.shape, val_input.shape)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    "dt.fit(sub_input, sub_target)\n",
    "print(dt.score(sub_input, sub_target))\n",
    "print(dt.score(val_input, val_target))\n",
    "#As we can see, It is overfitting to train_set. We have to change parameter for find better model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Cross validation\n",
    "#devide train set into k folds\n",
    "#Each fold is used validation data in turn. Remain k-1 folds are used train_set.\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(dt, train_input, train_target)\n",
    "print(scores)\n",
    "#cross_validate() method return dictionary form(key-value form), this method default do 5-cross validation, we can change k value using cv parameter\n",
    "    #fit_time : time taken to train model\n",
    "    #score_time : time taken to scoring\n",
    "    #test_score : validation_score for each fold  (scored by validation data)\n",
    "    #train_score : train score for each fold      (scored by train data) (it is printed when return_train_score=True)\n",
    "#Final score is average of test_score (It's name is test_score but don't forget, it is score of validation fold)\n",
    "import numpy as np\n",
    "print('final score of model : ', np.mean(scores['test_score']))\n",
    "\n",
    "#this block is same with upper block --> 5-cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())\n",
    "print(np.mean(scores['test_score']))\n",
    "\n",
    "#10-cross validation\n",
    "splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "scores = cross_validate(dt, train_input, train_target, cv = splitter)\n",
    "print(np.mean(scores['test_score']))\n",
    "\n",
    "\n",
    "##Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}\n",
    "#make dictionary. Key value is name of be serched parameter, value is range of parameter can get\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs=-1)\n",
    "gs.fit(train_input, train_target)\n",
    "#사이킷런의 그리드서치는 찾아낸 최적의 파라미터 조합으로 전체 훈련 세트에서 자동으로 다시 훈련을 진행한다. 그리고 그 모델을 best_estimator_에 저장한다.\n",
    "dt = gs.best_estimator_\n",
    "print(dt.score(train_input, train_target))\n",
    "#Best hyper parameters that grid search class found is contatined in best_params_\n",
    "print(gs.best_params_)\n",
    "#average score of cross_validation for each hyperparameter candidates is contained in 'mean_test_score' key value of gs.cv_results_ dictionary\n",
    "print(gs.cv_results_['mean_test_score'])\n",
    "\n",
    "best_index = np.argmax(gs.cv_results_['mean_test_score'])\n",
    "print(gs.cv_results_['params'][best_index])\n",
    "\n",
    "\n",
    "#use grid search on more complex parameters - find best combination of three hyperparameters\n",
    "params = {\n",
    "    'min_impurity_decrease' : np.arange(0.0001, 0.001, 0.0001),\n",
    "    'max_depth' : range(5, 20, 1),\n",
    "    'min_samples_split' : range(2, 200, 10)\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs=-1)\n",
    "gs.fit(train_input, train_target)\n",
    "print(gs.best_params_)\n",
    "print(np.max(gs.cv_results_['mean_test_score']))\n",
    "\n",
    "\n",
    "\n",
    "##Random Search\n",
    "from scipy.stats import uniform, randint\n",
    "#uniform - 실수값  랜덤하게 뽑음\n",
    "#randint - 정수값  랜덤하게 뽑음. --> C/C++의 rand()랑 동일하게 작동 하는듯?\n",
    "\n",
    "rgen = randint(0,10)\n",
    "rgen.rvs(10)\n",
    "\n",
    "np.unique(rgen.rvs(1000), return_counts=True)\n",
    "\n",
    "ugen = uniform(0,1)\n",
    "ugen.rvs(10)\n",
    "\n",
    "\n",
    "#select kind of parameters that model will find\n",
    "params = {\n",
    "    'min_impurity_decrease' : uniform(0.0001, 0.001),\n",
    "    'max_depth' : randint(20, 50),\n",
    "    'min_samples_split' : randint(2, 25),\n",
    "    'min_samples_leaf' : randint(1, 25),\n",
    "    }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)\n",
    "gs.fit(train_input, train_target)\n",
    "print(gs.best_params_)\n",
    "print(np.max(gs.cv_results_['mean_test_score']))\n",
    "dt = gs.best_estimator_\n",
    "print(dt.score(test_input, test_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
